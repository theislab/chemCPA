{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Plots of the latent drug embedding space of CCPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch[11:46:44] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /home/icb/simon.boehm/miniconda3/envs/chemical_CPA/lib/python3.7/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.1.so: cannot open shared object file: No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import umap.plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sn\n",
    "import seml\n",
    "from compert.data import Dataset, drug_names_to_once_canon_smiles, canonicalize_smiles\n",
    "from compert.model import ComPert\n",
    "from compert.embedding import get_chemical_representation\n",
    "import torch\n",
    "\n",
    "matplotlib.style.use(\"fivethirtyeight\")\n",
    "matplotlib.style.use(\"seaborn-talk\")\n",
    "matplotlib.rcParams['font.family'] = \"monospace\"\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "matplotlib.pyplot.rcParams['savefig.facecolor'] = 'white'\n",
    "sn.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints_dir = Path(\"/storage/groups/ml01/projects/2021_chemicalCPA_leon.hetzel/sweeps/checkpoints\")\n",
    "# model is specified via the model hash (this is some seq2seq model that I picked randomly)\n",
    "model_hash = \"14b0557bb351b024fa5abcaae90be37c\"\n",
    "model_checkp = (model_checkpoints_dir / (model_hash + \".pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the config used to train the model from the mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241f7b0ba9f5488ca4e577e2a10f17bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73363c6eb8d54c42a75289a03b7ff276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seml_collection=\"sciplex_hparam\"\n",
    "results_df = seml.get_results(\n",
    "    seml_collection,\n",
    "    to_data_frame=True,\n",
    "    fields=[\"config\", \"config_hash\"],\n",
    "    states=[\"COMPLETED\"],\n",
    "    filter_dict={\"config_hash\": model_hash}\n",
    ")\n",
    "experiment = results_df.apply(\n",
    "    lambda exp: {\n",
    "        \"hash\": exp[\"config_hash\"],\n",
    "        \"seed\": exp[\"config.seed\"],\n",
    "        \"_id\": exp[\"_id\"],\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "assert len(experiment) == 1\n",
    "experiment = experiment[0]\n",
    "collection = seml.database.get_collection(seml_collection)\n",
    "config = collection.find_one({\"_id\": experiment[\"_id\"]})[\"config\"]\n",
    "assert config[\"dataset\"][\"data_params\"][\"use_drugs_idx\"]\n",
    "assert config[\"model\"][\"additional_params\"][\"doser_type\"] == \"amortized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset that was used by the model (this could be modified to just load the subset of the dataset) and extract the SMILES + pathways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_key = config[\"dataset\"][\"data_params\"][\"perturbation_key\"]\n",
    "smiles_key = config[\"dataset\"][\"data_params\"][\"smiles_key\"]\n",
    "dataset = sc.read(config[\"dataset\"][\"data_params\"][\"dataset_path\"])\n",
    "\n",
    "# this is how the `canon_smiles_unique_sorted` is generated inside compert.data.Dataset\n",
    "# we need to have the same ordering of SMILES, else the mapping to pathways will be off\n",
    "# when we load the Vanilla embedding. For the other embeddings it's not as important.\n",
    "drugs_names = np.array(dataset.obs[perturbation_key].values)\n",
    "drugs_names_unique = set()\n",
    "for d in drugs_names:\n",
    "    [drugs_names_unique.add(i) for i in d.split(\"+\")]\n",
    "drugs_names_unique_sorted = np.array(sorted(drugs_names_unique))\n",
    "canon_smiles_unique_sorted = drug_names_to_once_canon_smiles(\n",
    "    list(drugs_names_unique_sorted), dataset, perturbation_key, smiles_key\n",
    ")\n",
    "\n",
    "smiles_to_pathway_map = {canonicalize_smiles(smiles): pathway for smiles,pathway in dataset.obs.groupby(\n",
    "    [config[\"dataset\"][\"data_params\"][\"smiles_key\"], \"pathway_level_1\"]).groups.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embedding that was used by the model. The embedding is returned in the same order as the smiles. For Vanilla load the embedding from the `state_dict` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=config[\"model\"][\"embedding\"][\"model\"]\n",
    "if embedding_model == \"vanilla\":\n",
    "    embedding = None\n",
    "else:\n",
    "    embedding = get_chemical_representation(\n",
    "        smiles=canon_smiles_unique_sorted,\n",
    "        embedding_model=config[\"model\"][\"embedding\"][\"model\"],\n",
    "        data_dir=config[\"model\"][\"embedding\"][\"directory\"],\n",
    "        device=\"cuda\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict, cov_state_dicts, init_args, history = torch.load(model_checkp)\n",
    "if embedding_model != \"vanilla\":\n",
    "    state_dict.pop(\"drug_embeddings.weight\")\n",
    "model = ComPert(**init_args, drug_embeddings=embedding)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomp_keys = model.load_state_dict(state_dict, strict=False)\n",
    "if embedding_model == \"vanilla\":\n",
    "    assert len(incomp_keys.unexpected_keys) == 0 and len(incomp_keys.missing_keys) == 0\n",
    "else:\n",
    "    # make sure we didn't accidentally load the embedding from the state_dict\n",
    "    torch.testing.assert_allclose(model.drug_embeddings.weight, embedding.weight)\n",
    "    assert len(incomp_keys.missing_keys) == 1 and \"drug_embeddings.weight\" in incomp_keys.missing_keys\n",
    "    assert len(incomp_keys.unexpected_keys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drugs_idx = torch.tensor(list(range(len(embedding.weight))))\n",
    "# TODO Check whether 1.0 is actually the max dosage.\n",
    "dosages = torch.ones((len(embedding.weight),))\n",
    "with torch.no_grad():\n",
    "    # scaled the drug embeddings using the doser\n",
    "    scaled_embeddings = model.compute_drug_embeddings_(drugs_idx=all_drugs_idx, dosages=dosages)\n",
    "    # apply drug embedder\n",
    "    transf_embeddings = model.drug_embedding_encoder(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_neighbors=8, min_dist=0.15).fit_transform(transf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important to use the same ordering of SMILES as was used for getting the embedding!\n",
    "pathway = [smiles_to_pathway_map[s] for s in canon_smiles_unique_sorted]\n",
    "sn.scatterplot(x=mapper[:,0], y=mapper[:,1], hue=pathway, palette=\"tab20\")\n",
    "bbox = (1, -0.1)\n",
    "plt.legend(bbox_to_anchor=bbox)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}